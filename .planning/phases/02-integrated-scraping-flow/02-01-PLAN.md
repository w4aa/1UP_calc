---
phase: 02-integrated-scraping-flow
plan: 01
type: execute
---

<objective>
Refactor scraping orchestration so BetPawa 1x2 odds changes trigger multi-bookmaker scraping, implementing the intended workflow where BetPawa acts as the primary change detector for all bookmaker data collection.

Purpose: Align implementation with PROJECT.md workflow where BetPawa 1x2 changes drive all scraping decisions, ensuring efficient data collection focused on actual odds movements.
Output: Sequential scraping flow with BetPawa as change detector, conditional scraping for Sporty/Bet9ja based on BetPawa 1x2 changes.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-change-detection/01-01-SUMMARY.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/STRUCTURE.md
@src/unified_scraper.py
@src/db/manager.py

**Tech stack available:** Python 3.9+, asyncio, SQLite (DatabaseManager), existing scrapers (Sporty/Pawa/Bet9ja)
**Established patterns:**
- Per-bookmaker scraping modules with async/await
- DatabaseManager for all DB operations
- Snapshot-based 1x2 change detection (Phase 1)
- Parallel tournament processing with semaphores

**Constraining decisions:**
- Phase 1: Use market_snapshots table as source of truth for odds comparison
- Phase 1: Keep per-bookmaker change detection architecture (now needs to change for Phase 2)
- Phase 1: 0.01 tolerance for odds change detection

**Current architecture (from Phase 1 summary):**
- Each bookmaker checks its own 1x2 odds independently and scrapes if changed
- Parallel execution: Sporty, Pawa, Bet9ja run concurrently within tournament
- Phase 1 noted: "If BetPawa-only triggering is critical, it would require refactoring scrapers to run sequentially with BetPawa first"

**Required change (from PROJECT.md workflow):**
- BetPawa checks 1x2 first
- If BetPawa 1x2 changed → scrape ALL bookmakers for that event
- If unchanged → skip event entirely
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BetPawa change detection phase</name>
  <files>src/unified_scraper.py</files>
  <action>
Create new method `_check_betpawa_changes_for_tournament(tournament, force)` that:
- Fetches BetPawa events for the tournament (using existing BetpawaEventsScraper)
- For each event with Sportradar ID:
  - Fetches 1x2 market odds via BetpawaMarketsScraper
  - Calls `db.check_1x2_odds_changed(sportradar_id, bookmaker='pawa', ...)`
  - If changed (or force=True): Add to changed_events dict with 1x2 odds
  - If unchanged: Skip event
- Returns: Dict[sportradar_id] = {'home_odds': float, 'draw_odds': float, 'away_odds': float, 'event': PawaEvent}
- Logs summary: "BetPawa change detection: X events found, Y have 1x2 changes"

This isolates BetPawa's role as change detector before any multi-bookmaker scraping begins.

IMPORTANT: Do NOT scrape all markets yet - only fetch events and 1x2 odds for change detection. Full market scraping happens in later task when triggered.
  </action>
  <verify>
Method exists with correct signature. Dry-run test: Check method can be called with tournament dict, returns dict format.
  </verify>
  <done>
`_check_betpawa_changes_for_tournament` method exists, returns Dict[sportradar_id] with changed events and their 1x2 odds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor tournament orchestration to sequential BetPawa-triggered flow</name>
  <files>src/unified_scraper.py</files>
  <action>
Modify `_process_tournament` method to implement BetPawa-first sequential flow:

1. **BetPawa Change Detection Phase** (first):
   - Call `_check_betpawa_changes_for_tournament(tournament, force)`
   - Store result: `changed_events: Dict[sportradar_id] = {...}`
   - If no changes and not force: Log "No BetPawa 1x2 changes, skipping tournament" and return early
   - Update BetPawa cached 1x2 odds in DB for all changed events (via db.update_1x2_odds)

2. **Conditional Multi-Bookmaker Scraping** (if changes detected):
   - Build filter set: `changed_sportradar_ids = set(changed_events.keys())`
   - Run three scrapers in parallel (existing pattern) BUT pass filter:
     - `_scrape_sportybet(tournament, force, filter_sportradar_ids=changed_sportradar_ids)` (if scrape_sporty)
     - `_scrape_betpawa(tournament, force, filter_sportradar_ids=changed_sportradar_ids)` (if scrape_pawa)
     - `_scrape_bet9ja(tournament, force, filter_sportradar_ids=changed_sportradar_ids)` (if bet9ja enabled)

3. **Snapshot creation** (unchanged):
   - Keep existing snapshot creation logic after scrapers complete

Remove existing per-bookmaker change detection from `_scrape_sportybet` and `_scrape_bet9ja` (they now just scrape filtered events without checking).

Keep `_scrape_betpawa` logic for full market scraping (the change detection method only fetched 1x2, now fetch all markets).

Why this approach: BetPawa becomes gatekeeper, other scrapers become followers. Cleaner separation of concerns.
  </action>
  <verify>
`_process_tournament` calls BetPawa change detection first, skips if no changes, passes filter to other scrapers. No per-bookmaker change checks remain in Sporty/Bet9ja.
  </verify>
  <done>
Tournament orchestration implements sequential flow: BetPawa checks → conditional multi-bookmaker scraping → snapshots. Early return if no changes.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update scraper methods to accept and honor event filter</name>
  <files>src/unified_scraper.py</files>
  <action>
Modify scraper method signatures and logic:

1. **`_scrape_sportybet(tournament, force, filter_sportradar_ids: Optional[Set[str]] = None)`**:
   - Add `filter_sportradar_ids` parameter (default None for backward compat)
   - After fetching events: If filter_sportradar_ids provided, skip events not in filter
   - Remove internal `check_1x2_odds_changed` call and early return logic (BetPawa already decided)
   - Remove `update_1x2_odds` call (BetPawa already updated)
   - Keep all market scraping logic unchanged

2. **`_scrape_betpawa(tournament, force, filter_sportradar_ids: Optional[Set[str]] = None)`**:
   - Add `filter_sportradar_ids` parameter
   - After fetching events: If filter provided, skip events not in filter
   - Remove internal `check_1x2_odds_changed` call (already done in change detection phase)
   - Remove `update_1x2_odds` call (already done in orchestration phase)
   - Keep all market scraping logic

3. **`_scrape_bet9ja(tournament, force, filter_sportradar_ids: Optional[Set[str]] = None)`**:
   - Add `filter_sportradar_ids` parameter
   - After fetching events: If filter provided, skip events not in filter
   - Bet9ja never had change detection, just add filter support
   - Keep all existing logic

Log clearly when filtering: "Filtering to X changed events (from BetPawa trigger)" so it's obvious in output.

Why optional parameter: Allows `--force` mode to still scrape everything if filter is None, maintains flexibility.
  </action>
  <verify>
All three scraper methods accept filter_sportradar_ids parameter. When provided, they only scrape events in filter set. No internal change detection remains.
  </verify>
  <done>
Scraper methods accept event filter, honor it by skipping non-filtered events, internal change detection removed from Sporty/Pawa.
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `_check_betpawa_changes_for_tournament` method exists and returns changed events dict
- [ ] `_process_tournament` calls BetPawa change detection first, returns early if no changes
- [ ] `_process_tournament` passes filter_sportradar_ids to all scraper methods
- [ ] All scraper methods (`_scrape_sportybet`, `_scrape_betpawa`, `_scrape_bet9ja`) accept and honor filter parameter
- [ ] No per-bookmaker change detection remains in Sporty/Bet9ja scraper methods
- [ ] `python main.py --scrape` runs without errors (smoke test)
- [ ] Logs clearly show BetPawa-first flow: "BetPawa change detection: ...", "Filtering to X events"
</verification>

<success_criteria>

- All tasks completed
- BetPawa change detection runs first and builds changed events dict
- Tournament scraping skips early if no BetPawa 1x2 changes
- Sporty/Bet9ja only scrape events that BetPawa flagged as changed
- Scraper methods accept filter_sportradar_ids parameter
- Per-bookmaker change detection removed from Sporty/Bet9ja
- No errors when running full pipeline
- Clear logging shows new flow in action
</success_criteria>

<output>
After completion, create `.planning/phases/02-integrated-scraping-flow/02-01-SUMMARY.md`:

# Phase 2 Plan 1: Integrated Scraping Flow Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `src/unified_scraper.py` - Description of changes

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Phase 2 complete. Ready for Phase 3: Engine Configuration & Validation.

All verification checks passed:
- ✓ BetPawa-first change detection implemented
- ✓ Tournament orchestration uses sequential flow
- ✓ Scraper methods accept event filter
- ✓ No errors in smoke test
</output>
