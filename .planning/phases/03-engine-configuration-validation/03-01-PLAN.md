---
phase: 03-engine-configuration-validation
plan: 01
type: execute
---

<objective>
Add engine configuration system with probability skew controls, verify automatic execution, and implement duplicate calculation prevention.

Purpose: Enable calibration control for home/away probability adjustments and ensure engines run automatically only on new snapshots, avoiding redundant calculations.
Output: Config system with ±#.###0 precision skew parameters, verified automatic execution flow, and duplicate-safe calculation logic.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-change-detection/01-01-SUMMARY.md
@.planning/phases/02-integrated-scraping-flow/02-01-SUMMARY.md
@config/engine.yaml
@src/engine/runner.py
@src/engine/fts_calibrated_dp.py
@src/db/manager.py

**Tech stack available:** Python 3.9+, YAML config system, SQLite database, ThreadPoolExecutor
**Established patterns:** YAML-based configuration with ConfigLoader, database-driven workflow
**Constraining decisions:**
- Phase 1: market_snapshots table is source of truth for odds comparison
- Phase 2: BetPawa triggers all scraping via sequential flow

**Requirements from roadmap:**
- Add config system for engine probability skew (home/away adjustments with ±#.###0 precision)
- Verify automatic execution works correctly
- Ensure engines only calculate new snapshots (skip if sportradar_id + history_id already exists in engine_calculations)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add probability skew configuration to engine.yaml</name>
  <files>config/engine.yaml, src/config.py</files>
  <action>
Add a new `calibration` section to config/engine.yaml with home_skew and away_skew parameters. Format: ±#.###0 (5 decimal places precision, can be positive or negative). Default both to 0.0 (no adjustment).

Add method `get_engine_calibration_skew()` to ConfigLoader class in src/config.py that returns dict with 'home_skew' and 'away_skew' float values from the calibration section. Return {'home_skew': 0.0, 'away_skew': 0.0} if section missing.

Example config structure:
```yaml
calibration:
  # Probability adjustments applied to calculated 1UP probabilities
  # Positive values increase probability, negative values decrease
  # Range: ±0.10000 (±10 percentage points)
  home_skew: 0.00000
  away_skew: 0.00000
```

Do NOT modify existing engine logic yet - just add config structure and loader method.
  </action>
  <verify>
python -c "from src.config import ConfigLoader; c = ConfigLoader(); print(c.get_engine_calibration_skew())" returns {'home_skew': 0.0, 'away_skew': 0.0}
  </verify>
  <done>
- engine.yaml has calibration section with home_skew and away_skew parameters
- ConfigLoader has get_engine_calibration_skew() method
- Method returns dict with float values
- Default values are 0.0 for both parameters
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement duplicate calculation prevention in EngineRunner</name>
  <files>src/engine/runner.py</files>
  <action>
Modify EngineRunner._compute_event() to check if calculation already exists before computing.

Add a new parameter `scraping_history_id` to _compute_event() signature. At the start of the method, query database to check if calculations already exist for this sportradar_id + scraping_history_id combination:

```python
# Check if calculations already exist for this snapshot
if scraping_history_id:
    existing = self.db.get_calculation_for_snapshot(sportradar_id, scraping_history_id)
    if existing:
        logger.debug(f"Skipping {sportradar_id} - calculations already exist for history_id {scraping_history_id}")
        return []
```

Add the new method `get_calculation_for_snapshot(sportradar_id: str, scraping_history_id: int)` to DatabaseManager in src/db/manager.py that returns first matching row or None:

```python
def get_calculation_for_snapshot(self, sportradar_id: str, scraping_history_id: int) -> Optional[dict]:
    cursor = self.conn.cursor()
    cursor.execute("""
        SELECT id FROM engine_calculations
        WHERE sportradar_id = ? AND scraping_history_id = ?
        LIMIT 1
    """, (sportradar_id, scraping_history_id))
    row = cursor.fetchone()
    return dict(row) if row else None
```

Update all callers of _compute_event() in runner.py to pass scraping_history_id parameter (run_event, _run_events_parallel, _run_sessions_parallel).

This prevents duplicate calculations when engines are run multiple times on the same snapshot. The database query is cheap (indexed) and avoids expensive recalculation.
  </action>
  <verify>
Run python -c "from src.db.manager import DatabaseManager; db = DatabaseManager('data/bets.db'); db.connect(); print(hasattr(db, 'get_calculation_for_snapshot'))" returns True

Check that _compute_event signature includes scraping_history_id parameter
  </verify>
  <done>
- _compute_event() checks for existing calculations before computing
- DatabaseManager has get_calculation_for_snapshot() method
- All callers pass scraping_history_id to _compute_event()
- Duplicate calculations are skipped with debug log message
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify automatic engine execution after scraping</name>
  <files>src/unified_scraper.py</files>
  <action>
Check if unified_scraper.py calls engine execution after scraping completes. Search for "run_engines" or "EngineRunner" in the file.

If NOT found: Add automatic engine execution at the end of UnifiedScraper.run() method, after scraping completes. Import EngineRunner and call run_new_snapshots() to process only the newly created snapshots:

```python
from src.engine.runner import EngineRunner

# At end of run() method, after all scraping:
if not self.dry_run:  # Check for dry_run flag if it exists
    logger.info("Running engines on new snapshots...")
    engine_runner = EngineRunner(self.db, self.config)
    result = engine_runner.run_new_snapshots()
    logger.info(f"Engine execution complete: {result['calculations']} calculations")
```

If FOUND: Verify it's calling run_new_snapshots() (not run_all_events) to process only new snapshots. Update if needed.

The goal is automatic execution - trader runs scraper once, engines calculate automatically without separate command.
  </action>
  <verify>
grep -n "EngineRunner\|run_engines\|run_new_snapshots" src/unified_scraper.py shows engine execution call

python -c "import ast; tree = ast.parse(open('src/unified_scraper.py').read()); print('run_new_snapshots' in ast.dump(tree))" returns True
  </verify>
  <done>
- UnifiedScraper.run() calls engine execution after scraping
- Engines run automatically via run_new_snapshots()
- Only new snapshots are processed (duplicate-safe via Task 2)
- Log messages confirm automatic execution
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] config/engine.yaml has calibration section with home_skew and away_skew
- [ ] ConfigLoader.get_engine_calibration_skew() returns correct dict
- [ ] DatabaseManager.get_calculation_for_snapshot() method exists
- [ ] EngineRunner._compute_event() skips duplicate calculations
- [ ] UnifiedScraper.run() automatically executes engines after scraping
- [ ] Python syntax check passes: python -m py_compile src/config.py src/engine/runner.py src/db/manager.py src/unified_scraper.py
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No syntax errors introduced
- Duplicate calculation prevention works correctly
- Automatic engine execution integrated into scraping flow
- Config system ready for future calibration adjustments
  </success_criteria>

<output>
After completion, create `.planning/phases/03-engine-configuration-validation/03-01-SUMMARY.md`:

---
phase: 03-engine-configuration-validation
plan: 01
subsystem: engine
tags: [configuration, calibration, duplicate-prevention, automatic-execution]

# Dependency graph
requires:
  - phase: 02-integrated-scraping-flow
    provides: Sequential scraping flow triggered by BetPawa 1x2 changes
provides:
  - Probability skew configuration system for engine calibration
  - Duplicate calculation prevention via snapshot-based checking
  - Automatic engine execution after scraping completes
affects: [04-end-to-end-data-flow]

# Tech tracking
tech-stack:
  added: []
  patterns: [config-driven-calibration, duplicate-safe-calculations, automatic-pipeline]

key-files:
  created: []
  modified: [config/engine.yaml, src/config.py, src/engine/runner.py, src/db/manager.py, src/unified_scraper.py]

key-decisions:
  - "Probability skew uses ±#.###0 precision (5 decimals) for fine-grained calibration"
  - "Duplicate detection via sportradar_id + scraping_history_id unique check"
  - "Automatic execution via run_new_snapshots() for newly created snapshots only"

patterns-established:
  - "Config-driven engine calibration without code changes"
  - "Snapshot-aware duplicate prevention for idempotent calculations"

issues-created: []

# Metrics
duration: [TBD]
completed: [TBD]
---

# Phase 3 Plan 1: Engine Configuration & Validation Summary

**[One-liner describing what shipped]**

## Accomplishments

- Added calibration section to engine.yaml with home_skew and away_skew parameters (±#.###0 precision)
- Implemented ConfigLoader.get_engine_calibration_skew() for reading calibration config
- Added DatabaseManager.get_calculation_for_snapshot() for duplicate detection
- Implemented duplicate calculation prevention in EngineRunner._compute_event()
- Integrated automatic engine execution into UnifiedScraper.run() via run_new_snapshots()

## Files Created/Modified

- `config/engine.yaml` - Added calibration section with skew parameters
- `src/config.py` - Added get_engine_calibration_skew() method
- `src/db/manager.py` - Added get_calculation_for_snapshot() method
- `src/engine/runner.py` - Implemented duplicate checking in _compute_event()
- `src/unified_scraper.py` - Added automatic engine execution after scraping

## Decisions Made

1. **±#.###0 precision**: 5 decimal places for probability skew allows fine-grained adjustments (e.g., ±0.00500 = ±0.5 percentage points)
2. **Duplicate detection strategy**: Check sportradar_id + scraping_history_id combination to skip redundant calculations
3. **Automatic execution placement**: Call run_new_snapshots() at end of UnifiedScraper.run() to process only newly created snapshots
4. **Config-only calibration**: Skew parameters added to config but NOT yet applied in engine logic (deferred to later phase if needed)

## Issues Encountered

[Any problems and resolutions, or "None"]

## Deviations from Plan

[Any deviations from planned approach, or "None"]

## Next Phase Readiness

Phase 3 Plan 1 complete. Ready for Phase 4: End-to-End Data Flow.

All verification checks passed:
- ✓ calibration section exists in engine.yaml
- ✓ ConfigLoader.get_engine_calibration_skew() works correctly
- ✓ DatabaseManager.get_calculation_for_snapshot() implemented
- ✓ EngineRunner skips duplicate calculations
- ✓ UnifiedScraper automatically runs engines after scraping
- ✓ Python syntax checks pass

---
*Phase: 03-engine-configuration-validation*
*Completed: [Date]*
</output>
