---
phase: 04-end-to-end-data-flow
plan: 01
type: execute
---

<objective>
Validate and verify the complete tournament → events → check → scrape → calculate → store pipeline works correctly end-to-end with proper error handling and observability.

Purpose: Ensure all components (Phases 1-3) integrate correctly and the full workflow executes reliably from tournament selection through calculation storage.
Output: Verified working pipeline with validation report and any fixes for discovered integration issues.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/STRUCTURE.md

# Prior phase context
@.planning/phases/01-change-detection/01-01-SUMMARY.md
@.planning/phases/02-integrated-scraping-flow/02-01-SUMMARY.md
@.planning/phases/03-engine-configuration-validation/03-01-SUMMARY.md

# Key source files
@main.py
@src/unified_scraper.py
@src/engine/runner.py
@src/db/manager.py

**Tech stack available:** Python 3.9+, asyncio, playwright, httpx, numpy, pyyaml, sqlite3
**Established patterns:**
- BetPawa-first sequential scraping flow with change detection
- Config-driven engine calibration
- Snapshot-aware duplicate prevention for calculations
- Automatic engine execution after scraping completes

**Constraining decisions:**
- Phase 1: Use market_snapshots table as source of truth for odds comparison
- Phase 2: BetPawa acts as change detector, triggering all bookmaker scraping
- Phase 3: Automatic execution via run_new_snapshots() for newly created snapshots only

**Current integration status:**
The pipeline is already fully integrated:
1. Tournament config → Database (tournaments table synced automatically during _process_tournament)
2. BetPawa change detection → Conditional scraping trigger
3. Multi-bookmaker scraping → Market snapshots
4. Engine runner → Automatic execution on new snapshots
5. Results → engine_calculations table

**Phase 4 focus:** Validation and verification, not new implementation.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end validation test</name>
  <files>tests/test_end_to_end_flow.py</files>
  <action>
    Create a comprehensive integration test that validates the full pipeline:

    1. Setup phase:
       - Clear test data from database (or use separate test DB)
       - Select one enabled tournament from config (e.g., Premier League)

    2. Execute full pipeline:
       - Call UnifiedScraper.run() with scrape_sporty=True, scrape_pawa=True, force=False
       - This triggers: tournaments sync → BetPawa change detection → multi-bookmaker scraping → snapshots → engines

    3. Validate each stage:
       - Tournament exists in tournaments table
       - Events fetched and stored in events table (check count > 0)
       - Market snapshots created (check market_snapshots table)
       - Scraping history records exist
       - Engine calculations created (check engine_calculations table)

    4. Test partial failure handling:
       - Simulate Sportybet scraper failure (mock exception)
       - Verify: BetPawa data still collected, snapshots still created
       - Verify: Pipeline continues despite individual scraper failure

    5. Test change detection optimization:
       - Run pipeline twice on same data (without force flag)
       - First run: should scrape and calculate
       - Second run: should detect no changes, skip scraping, no new snapshots
       - Verify: engines not triggered when no new snapshots

    Use pytest for test structure. Use existing test patterns from tests/test_system.py.
    Assert on database counts and record existence, not exact values.
    Keep test runtime under 2 minutes by using single tournament.
  </action>
  <verify>pytest tests/test_end_to_end_flow.py -v passes all assertions</verify>
  <done>Test file exists, runs successfully, validates all 5 scenarios, completes in under 2 minutes</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>End-to-end validation test covering full pipeline flow and partial failure scenarios</what-built>
  <how-to-verify>
    1. Run: pytest tests/test_end_to_end_flow.py -v -s
    2. Verify: All test stages pass (tournament sync, event fetching, snapshot creation, engine execution)
    3. Verify: Partial failure test shows resilience (one scraper fails, others continue)
    4. Verify: Change detection optimization test shows no re-scraping when data unchanged
    5. Check: Test completes in reasonable time (under 2 minutes)
    6. Review: Terminal output shows clear stage progression and validation messages
  </how-to-verify>
  <resume-signal>Type "approved" to continue, or describe any issues found</resume-signal>
</task>

<task type="auto">
  <name>Task 2: Add pipeline observability improvements</name>
  <files>src/unified_scraper.py, main.py</files>
  <action>
    Enhance logging and progress tracking for better observability:

    1. In UnifiedScraper._process_tournament (around line 355):
       - Add log message after tournament upsert: "Tournament synced to database: [name]"
       - This confirms tournaments table population

    2. In UnifiedScraper._print_stats method:
       - Add summary line at end: "Pipeline complete: X tournaments, Y events, Z snapshots, W calculations"
       - This gives immediate feedback on full pipeline execution

    3. In main.py run_engines function (around line 60):
       - Add log before checking unprocessed sessions: "Checking for new snapshots to calculate..."
       - If no unprocessed sessions, log: "No new snapshots found - all calculations up to date"
       - This clarifies when engines are skipped due to no new data

    4. Add pipeline stage markers in UnifiedScraper.run:
       - Before tournaments loop: logger.info("STAGE 1: Tournament Processing")
       - After tournaments loop, before engines: logger.info("STAGE 2: Engine Calculations")
       - This makes the two-stage flow explicit in logs

    DO NOT change functionality, only add logging statements.
    DO NOT add verbose debug logs, only high-level progress markers.
    Use existing logger.info() style, keep messages concise (under 80 chars).
  </action>
  <verify>
    1. Run: python main.py --help (verify no errors)
    2. Check: New log statements present in source files
    3. Verify: Log messages follow existing format and style
    4. No functionality changes, only observability improvements
  </verify>
  <done>Logging statements added, code syntax valid, messages clear and concise, no behavior changes</done>
</task>

<task type="auto">
  <name>Task 3: Create end-to-end flow documentation</name>
  <files>.planning/phases/04-end-to-end-data-flow/FLOW.md</files>
  <action>
    Create a visual documentation of the complete pipeline flow:

    Structure:

    # End-to-End Data Flow

    ## Overview
    Brief description of what the pipeline does (tournament → calculation)

    ## Pipeline Stages

    ### Stage 1: Tournament Configuration
    - Source: config/tournaments.yaml
    - Process: UnifiedScraper syncs to tournaments table
    - Output: Tournament records in database

    ### Stage 2: BetPawa Change Detection
    - For each enabled tournament:
      - Fetch BetPawa events and 1x2 odds
      - Compare against latest market_snapshot
      - Identify changed events (sportradar_id list)
    - Early return: If no changes and not force mode, skip tournament

    ### Stage 3: Multi-Bookmaker Scraping
    - Only for events with BetPawa 1x2 changes
    - Parallel scraping: Sportybet + BetPawa + Bet9ja
    - Event filtering: Only changed sportradar_ids
    - Output: Markets stored, events updated

    ### Stage 4: Snapshot Creation
    - Per tournament: create_snapshots_for_matched_events()
    - Creates scraping_history record per match
    - Creates market_snapshots for all markets
    - Links snapshots to history_id

    ### Stage 5: Engine Execution
    - Automatic: run_new_snapshots() called by UnifiedScraper
    - Processes only new snapshots (duplicate prevention)
    - Parallel execution: ThreadPoolExecutor
    - Output: engine_calculations records

    ## Error Handling
    - Scraper failures: Isolated per bookmaker (gather with return_exceptions=True)
    - Database errors: Logged and bubbled to top-level
    - Partial success: Pipeline continues if one scraper fails

    ## Performance Characteristics
    - Concurrent tournament processing (semaphore-limited)
    - Shared browser for Sportybet (connection pooling)
    - Change detection optimization (skip unchanged tournaments)
    - Duplicate calculation prevention (snapshot-aware)

    ## Entry Points
    - Full pipeline: python main.py
    - Scrape only: python main.py --scrape
    - Engines only: python main.py --engines
    - Force mode: python main.py --force (skip change detection)

    Use markdown format, keep concise (under 150 lines).
    Focus on WHAT happens at each stage, not HOW (code details).
    Include file references with line numbers for key integration points.
  </action>
  <verify>
    1. File exists: .planning/phases/04-end-to-end-data-flow/FLOW.md
    2. Contains all 5 pipeline stages clearly documented
    3. Includes error handling and performance sections
    4. Markdown formatting valid (can be rendered)
    5. Under 150 lines total
  </verify>
  <done>FLOW.md exists, all stages documented, clear visual flow, proper markdown</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] End-to-end test exists and passes (tests/test_end_to_end_flow.py)
- [ ] Test validates all 5 scenarios (full pipeline, partial failure, change detection optimization)
- [ ] Observability improvements merged (logging statements added)
- [ ] FLOW.md documentation created
- [ ] No new errors or warnings introduced
- [ ] Pipeline executes successfully from tournament selection through calculation storage
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- End-to-end test demonstrates pipeline integrity
- Observability improvements make pipeline stages visible in logs
- FLOW.md provides clear documentation for stakeholders and developers
- Phase 4 complete, ready for Phase 5 (Data Quality Validation)
  </success_criteria>

<output>
After completion, create `.planning/phases/04-end-to-end-data-flow/04-01-SUMMARY.md`:

# Phase 4 Plan 1: End-to-End Data Flow Summary

**[Substantive one-liner - what was validated/fixed]**

## Performance

- **Duration:** [X] min
- **Started:** [timestamp]
- **Completed:** [timestamp]
- **Tasks:** 3
- **Files modified:** [count]

## Accomplishments

- [Key validation results]
- [Integration issues found and fixed (if any)]
- [Observability improvements added]

## Task Commits

Each task committed atomically:

1. **Task 1: Create end-to-end validation test** - [hash] (test)
2. **Task 2: Add pipeline observability improvements** - [hash] (feat)
3. **Task 3: Create end-to-end flow documentation** - [hash] (docs)

## Files Created/Modified

- `tests/test_end_to_end_flow.py` - Comprehensive integration test
- `src/unified_scraper.py` - Observability improvements
- `main.py` - Engine logging enhancements
- `.planning/phases/04-end-to-end-data-flow/FLOW.md` - Pipeline documentation

## Decisions Made

[Any architectural or implementation decisions made during validation]

## Issues Encountered

[Problems discovered during testing, how they were resolved, or deferred to ISSUES.md]

## Deviations from Plan

[None, or describe any changes to approach]

## Next Phase Readiness

Phase 4 Plan 1 complete. Ready for Phase 5: Data Quality Validation.

All verification checks passed:
- ✓ End-to-end test passes all 5 scenarios
- ✓ Observability improvements deployed
- ✓ FLOW.md documentation complete
- ✓ No new errors introduced
- ✓ Pipeline executes successfully end-to-end

---
*Phase: 04-end-to-end-data-flow*
*Completed: [date]*
</output>
